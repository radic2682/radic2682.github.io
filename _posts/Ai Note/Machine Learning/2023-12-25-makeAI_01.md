---
title: "[강화학습] Q-learning"
thumbnail: 
layout: post
author: Sun Hong
categories:
  - Ai Note
  - Machine Learning
tags:
  - tag1
  - tag2
  - tag3
excerpt: 강화학습, Q-learning, Bellman Equation
sticker: emoji//1f4d6
use_math: "true"
---
# 1. 강화학습이란?
강화학습(Reinforcement Learning, RL)은 기계 학습의 한 분야로, 에이전트가 환경과 상호 작용하며 어떤 목표를 달성하기 위해 최적의 행동 방식을 스스로 학습하는 과정입니다. 에이전트는 환경에 대한 행동을 취하고, 그 결과로 보상이나 처벌을 받으며, 이러한 경험을 통해 어떤 행동이 더 나은 결과를 가져오는지 학습합니다. 목표는 시간이 지남에 따라 누적되는 보상을 최대화하는 것입니다.

`Environment` - 인공지능이 겪는 환경
`Agent` - 인공지능 (환경을 탐색하며 학습하는 존재)

`Environment`에서 `Agent`가 특정 행동(Action)을 수행하면 <u>상태를 계속해서 변화</u>시킵니다.
( Good -> 보상! )

# 2. The Bellman Equation
벨만 방정식(Bellman Equation)은 동적 프로그래밍과 강화학습에서 중요한 재귀적 관계를 나타내는 방정식입니다. 이 방정식은 미국의 수학자 리처드 E. 벨만의 이름을 따서 명명되었습니다. 강화학습의 맥락에서 벨만 방정식은 주어진 상태의 가치나 주어진 상태에서 취한 행동의 가치를 예상 미래 보상을 기반으로 정량화하는 방법을 제공합니다.

![image](https://github.com/radic2682/radic2682.github.io/assets/11177959/2fcb1b9c-3310-4860-8ea5-6e83b8cfc706)

`Agent`를 가만히 내버려 두면서 목표에 도달하도록 합니다.
( 가장 가치있는 경로를 탐색하며, 목표에 도달하려고 합니다. )

## Bellman Equation
상태 가치 함수를 간략화한 식은 일반적으로 다음과 같이 표현됩니다.

$V(s) = max(R(s, a) + γV(s'))$

`V(s)` 상태 s에 대한 가치 함수
`R(s, a)` 보상

`s` 현재 상태
`s'` 앞으로의 상태 상태
`a` Action으로 행동

`γ` 할인율로, 미래 보상을 현재 가치로 변환하는 데 사용됩니다.

=> 이러한 함수의 관점으로 볼 때, 목표에 가까워질수록 <u>최대값에 가까운 보상</u>을 받습니다.

![image](https://github.com/radic2682/radic2682.github.io/assets/11177959/6ad4b2ad-09f3-43c9-9c1c-ffe2e549dbd9)

## Plan
강화학습의 맥락에서 "계획(plan)"은 미래의 기대 보상에 기반하여 결정을 내리는 과정을 의미합니다. 여기서 "계획"은 에이전트가 목표를 달성하기 위해 따르는 전략이나 정책을 말합니다. 강화학습에서 계획은 누적 보상을 최대화하기 위한 최적의 행동 순서를 결정하는 것을 포함합니다.

즉, Agent의 행동을 보여주는 Map ( <u>어떤 선택을 할 지 화살표로 나타냄</u> )

# 3. Markov Decision Process (MDP)
마르코프 결정 과정(Markov Decision Process, MDP)은 결정을 내려야 하는 확률적 환경을 모델링하는 데 사용되는 수학적 프레임워크입니다. 강화학습과 관련하여, MDP는 에이전트가 상호 작용하는 환경을 정형화하고, 에이전트가 시간에 따라 결정을 내리면서 목표를 달성하기 위한 최적의 전략을 학습할 수 있는 기반을 제공합니다.

**Deterministic Search**
Agent가 위로 이동하려고 했다면, 무조건 위로 올라가게 됩니다.

**Non-Deterministic Search**
Agent가 위로 이동하려고 했다면, 위 뿐만 아니라 다른 곳으로도 이동합니다.
ex) 위 70%, 좌 5%, 우 25%

- 마르코프 속성: 현재 상태에 의해서만 결정!! 과거의 사건과는 연관이 없습니다.
- 마르코프 과정: 이러한 마르코프 속성을 가지는 과정 (환경에 임의의 요소가 있다는 것)
- 마르코프 의사 결정 과정: Agent가 이 환경에서 뭘 해야 하는 지 이해하려고 사용하는 툴

`마르코프 과정 != 마르코프 의사 결정 과정`

## 재정의한 Bellman Equation

$V(s) = max(R(s, a) + γ∑P(s, a, s')V(s'))$

가치 함수에 대한 `평균값`을 넣어줍니다.

# 4. Policy vs. Plan
